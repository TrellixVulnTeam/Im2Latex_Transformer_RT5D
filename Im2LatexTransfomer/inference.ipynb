{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from thanh_transformers.x_transformers import TransformerWrapper, Decoder\n",
    "from thanh_transformers.autoregressive_wrapper import top_k, top_p, entmax, ENTMAX_ALPHA\n",
    "from thanh.models.vision_transformer_hybrid import HybridEmbed\n",
    "from thanh.models.resnetv2 import ResNetV2\n",
    "from thanh.models.layers import StdConv2dSame\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from utils import parse_args, pad, post_process, token2str\n",
    "from test_transform import test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_size(image, max_dimensions=None, min_dimensions=None):\n",
    "    if max_dimensions is not None:\n",
    "        ratios = [a/b for a, b in zip(image.size, max_dimensions)]\n",
    "        if any([r > 1 for r in ratios]):\n",
    "            size = np.array(image.size)//max(ratios)\n",
    "            image = image.resize(size.astype(int), Image.BILINEAR)\n",
    "    if min_dimensions is not None:\n",
    "        if any([s < min_dimensions[i] for i, s in enumerate(image.size)]):\n",
    "            padded_im = Image.new('L', min_dimensions, 255)\n",
    "            padded_im.paste(image, image.getbbox())\n",
    "            image = padded_im\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LatexModel import get_model\n",
    "# from thanh_transformers.x_transformers import TransformerWrapper, Decoder\n",
    "# from thanh.models.vision_transformer_hybrid import HybridEmbed\n",
    "# from thanh.models.resnetv2 import ResNetV2\n",
    "# from thanh.models.layers import StdConv2dSame\n",
    "\n",
    "# from CustomVisionTransformer import CustomVisionTransformer\n",
    "# from CustomARWrapper import CustomARWrapper\n",
    "# from LatexModel import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import Munch\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "arguments = None\n",
    "if arguments is None:\n",
    "    arguments = Munch({'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': True, 'no_resize': False})\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "with open(arguments.config, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = parse_args(Munch(params))\n",
    "args.update(**vars(arguments))\n",
    "args.wandb = False\n",
    "args.device = \"cpu\"\n",
    "# args.device = 'cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexModel = get_model(args=args)\n",
    "\n",
    "latexModel.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "image_resizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=args.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = latexModel.encoder, latexModel.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x64 at 0x7F963930A290>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "image_equation = Image.open(\"./sample/10024a5ccf.png\")\n",
    "print(np.shape(image_equation))\n",
    "image_equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'V_{2}(x)=-{\\\\frac{1}{2}}x^{2}-{\\\\frac{\\\\mu^{2}}{2x^{2}}},'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_image = minmax_size(pad(image_equation), args.max_dimensions, args.min_dimensions)\n",
    "np_image = np.array(pad(np_image).convert('RGB'))\n",
    "t = test_transform(image=np_image)['image'][:1].unsqueeze(0)\n",
    "tensor_image = t.to(args.device)\n",
    "print(np.shape(tensor_image))\n",
    "\n",
    "with torch.no_grad():\n",
    "    latexModel.eval()\n",
    "    device = args.device\n",
    "    encoded = encoder_model(tensor_image)\n",
    "    start_tokens = torch.LongTensor([args.bos_token])[:, None].to(device)\n",
    "    encodeWeight = encoded.detach()\n",
    "    dec = decoder_model.generate(start_tokens, args.max_seq_len,\n",
    "                           eos_token=args.eos_token, context=encodeWeight, temperature=args.get('temperature', .25))\n",
    "    tokenString = token2str(dec, tokenizer)[0]\n",
    "    pred = post_process(tokenString)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transWrapper = decoder_model.net\n",
    "out = start_tokens\n",
    "x = out[:, -args.max_seq_len:]\n",
    "torch.onnx.export(transWrapper,               # model being run\n",
    "                  x,                          # model input (or a tuple for multiple inputs)context=encoder(im)\n",
    "                  \"latex_encode.onnx\",        # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,         # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,           # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,   # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],    # the model's input names\n",
    "                  output_names = ['output'],  # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeWeight = encoded.detach()\n",
    "# im = torch.empty(args.batchsize, args.channels, args.max_height, args.min_height, device=args.device).float()\n",
    "seq = torch.randint(0, args.num_tokens, (args.batchsize, args.max_seq_len), device=args.device).long()\n",
    "decoder_model(seq, context=encodeWeight).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = start_tokens.device\n",
    "# was_training = decoder_model.training\n",
    "# num_dims = len(start_tokens.shape)\n",
    "\n",
    "# if num_dims == 1:\n",
    "#     start_tokens = start_tokens[None, :]\n",
    "\n",
    "# b, t = start_tokens.shape\n",
    "\n",
    "# decoder_model.eval()\n",
    "# out = start_tokens\n",
    "# # mask = kwargs.pop('mask', None)\n",
    "# # if mask is None:\n",
    "# #     mask = torch.full_like(\n",
    "# #         out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "# for _ in range(seq_len):\n",
    "#     x = out[:, -self.max_seq_len:]\n",
    "#     mask = mask[:, -self.max_seq_len:]\n",
    "#     # print('arw:',out.shape)\n",
    "#     logits = self.net(x, mask=mask, **kwargs)[:, -1, :]\n",
    "\n",
    "#     if filter_logits_fn in {top_k, top_p}:\n",
    "#         filtered_logits = filter_logits_fn(logits, thres=filter_thres)\n",
    "#         probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "#     sample = torch.multinomial(probs, 1)\n",
    "\n",
    "#     out = torch.cat((out, sample), dim=-1)\n",
    "#     mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "#     if eos_token is not None and (torch.cumsum(out == eos_token, 1)[:, -1] >= 1).all():\n",
    "#         break\n",
    "\n",
    "# out = out[:, t:]\n",
    "\n",
    "# if num_dims == 1:\n",
    "#     out = out.squeeze(0)\n",
    "\n",
    "# decoder_model.net.train(was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153433/3275689409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_logits_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cao Hoc/Research Methodology/Im2Tex/running/thanh_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_embeddings, mask, return_mems, return_attn, mems, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtie_embedding\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;31m# memory tokens (like [cls]) from Memory Transformers paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mnum_memory_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_memory_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'epoch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ///////////////////////////\n",
    "start_tokens = torch.LongTensor([args.bos_token])[:, None].to(device)\n",
    "seq_len = args.max_seq_len\n",
    "eos_token=args.eos_token\n",
    "temperature=1.\n",
    "filter_logits_fn=top_k\n",
    "filter_thres=0.9\n",
    "# ///////////////////////////\n",
    "device = start_tokens.device\n",
    "was_training = decoder_model.net.training\n",
    "num_dims = len(start_tokens.shape)\n",
    "\n",
    "if num_dims == 1:\n",
    "    start_tokens = start_tokens[None, :]\n",
    "\n",
    "b, t = start_tokens.shape\n",
    "\n",
    "decoder_model.net.eval()\n",
    "out = start_tokens\n",
    "mask = args.pop('mask', None)\n",
    "if mask is None:\n",
    "    mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "for _ in range(seq_len):\n",
    "    x = out[:, -args.max_seq_len:]\n",
    "    mask = mask[:, -args.max_seq_len:]\n",
    "\n",
    "    logits = decoder_model.net(x, mask=mask, **args)[:, -1, :]\n",
    "\n",
    "    if filter_logits_fn in {top_k, top_p}:\n",
    "        filtered_logits = filter_logits_fn(logits, thres = filter_thres)\n",
    "        probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "    elif filter_logits_fn is entmax:\n",
    "        probs = entmax(logits / temperature, alpha = ENTMAX_ALPHA, dim=-1)\n",
    "\n",
    "    sample = torch.multinomial(probs, 1)\n",
    "\n",
    "    out = torch.cat((out, sample), dim=-1)\n",
    "    mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "    if eos_token is not None and (sample == eos_token).all():\n",
    "        break\n",
    "\n",
    "out = out[:, t:]\n",
    "\n",
    "if num_dims == 1:\n",
    "    out = out.squeeze(0)\n",
    "\n",
    "decoder_model.net.train(was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make_dot(latexModel(im.to(device)), params=dict(latexModel.named_parameters()))\n",
    "# from CustomVisionTransformer import CustomVisionTransformer\n",
    "# backbone = ResNetV2(\n",
    "#     layers=args.backbone_layers, num_classes=0, global_pool='', in_chans=args.channels,\n",
    "#     preact=False, stem_type='same', conv_layer=StdConv2dSame)\n",
    "# min_patch_size = 2**(len(args.backbone_layers)+1)\n",
    "\n",
    "# def embed_layer(**x):\n",
    "#     ps = x.pop('patch_size', min_patch_size)\n",
    "#     assert ps % min_patch_size == 0 and ps >= min_patch_size, 'patch_size needs to be multiple of %i with current backbone configuration' % min_patch_size\n",
    "#     return HybridEmbed(**x, patch_size=ps//min_patch_size, backbone=backbone)\n",
    "\n",
    "# encoder = CustomVisionTransformer(img_size=(args.max_height, args.max_width),\n",
    "#                                     patch_size=args.patch_size,\n",
    "#                                     in_chans=args.channels,\n",
    "#                                     num_classes=0,\n",
    "#                                     embed_dim=args.dim,\n",
    "#                                     depth=args.encoder_depth,\n",
    "#                                     num_heads=args.heads,\n",
    "#                                     embed_layer=embed_layer\n",
    "#                                     ).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot = make_dot(encoder(im.to(device)), params=dict(encoder.named_parameters()))\n",
    "\n",
    "# dot.format = 'svg'\n",
    "# dot.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(encoder,               # model being run\n",
    "#                   im.to(device),                         # model input (or a tuple for multiple inputs)\n",
    "#                   \"latex_encode.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                   opset_version=11,          # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],   # the model's input names\n",
    "#                   output_names = ['output'], # the model's output names\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(encoder,               # model being run\n",
    "#                   im.to(device),                         # model input (or a tuple for multiple inputs)\n",
    "#                   \"latex_encode.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                   opset_version=11,          # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],   # the model's input names\n",
    "#                   output_names = ['output'], # the model's output names\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = torch.empty(args.batchsize, args.channels, args.max_height, args.min_height, device=args.device).float()\n",
    "# seq = torch.randint(0, args.num_tokens, (args.batchsize, args.max_seq_len), device=args.device).long()\n",
    "# decoder(seq, context=encoder(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(decoder,               # model being run\n",
    "#                   seq,                         # model input (or a tuple for multiple inputs)context=encoder(im)\n",
    "#                   \"latex_encode.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                   opset_version=11,          # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],   # the model's input names\n",
    "#                   output_names = ['output'], # the model's output names\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# writer.add_graph(encoder,im.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone = ResNetV2(\n",
    "#     layers=args.backbone_layers, num_classes=0, global_pool='', in_chans=args.channels,\n",
    "#     preact=False, stem_type='same', conv_layer=StdConv2dSame)\n",
    "\n",
    "# from zetane import TorchViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zetane import TorchViz\n",
    "\n",
    "# viz = TorchViz()\n",
    "# viz.create_torch_model(encoder, im.to(device))\n",
    "# viz.model_inference(im.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = torch.LongTensor([args.bos_token])[:, None].to(device)\n",
    "seq_len = 256\n",
    "eos_token = args.eos_token\n",
    "temperature = .2\n",
    "filter_logits_fn = top_k\n",
    "filter_thres = 0.9\n",
    "context = encodeWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_{2}(x)=-{\\\\frac{1}{2}}x^{2}-{\\\\frac{\\\\mu^{2}}{2x^{2}}},'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # out = start_tokens\n",
    "# # mask = args.pop('mask', None)\n",
    "# # if mask is None:\n",
    "# #     mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "# # x = out[:, -args.max_seq_len:]\n",
    "# # mask = mask[:, -args.max_seq_len:]\n",
    "# encodeWeight = encoded.detach()\n",
    "# # decoder_model.net(x, mask=mask, context=encodeWeight)\n",
    "\n",
    "# decoder_model(start_tokens,\n",
    "#               seq_len=args.max_seq_len,\n",
    "#               eos_token=args.eos_token,\n",
    "#               temperature=.2,\n",
    "#               context=encodeWeight)\n",
    "# tokenString = token2str(dec, tokenizer)[0]\n",
    "# pred = post_process(tokenString)\n",
    "# pred\n",
    "\n",
    "decoder_model(start_tokens,\n",
    "              seq_len=args.max_seq_len,\n",
    "              eos_token=args.eos_token,\n",
    "              temperature=.2,\n",
    "              context=encodeWeight)\n",
    "tokenString = token2str(dec, tokenizer)[0]\n",
    "pred = post_process(tokenString)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153433/3000337311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraced_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         )\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    963\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 \u001b[0margument_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             )\n\u001b[1;32m    967\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cao Hoc/Research Methodology/Im2Tex/running/CustomARWrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, start_tokens, seq_len, eos_token, temperature, filter_logits_fn, filter_thres, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 out, True, dtype=torch.bool, device=out.device)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "max_seq_len = args.max_seq_len\n",
    "traced_cell = torch.jit.trace(decoder_model, (start_tokens, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 41, 256])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "context must be passed in if cross_attend is set to True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153433/539366005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mexport_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# store the trained parameter weights inside the model file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# the ONNX version to export the model to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                   \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# whether to execute constant folding for optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                   )\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                         export_modules_as_functions)\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             custom_opsets=custom_opsets, export_modules_as_functions=export_modules_as_functions)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    722\u001b[0m                                 \u001b[0mfixed_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                                 dynamic_axes=dynamic_axes)\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;31m# TODO: Don't allocate a in-memory string for the protobuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0mtrace_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_states\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0mwarn_on_static_input_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mONNXTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0m_create_interpreter_name_lookup_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cao Hoc/Research Methodology/Im2Tex/running/CustomARWrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, start_tokens, seq_len, eos_token, temperature, filter_logits_fn, filter_thres, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# print('arw:',out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilter_logits_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cao Hoc/Research Methodology/Im2Tex/running/thanh_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_embeddings, mask, return_mems, return_attn, mems, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtie_embedding\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;31m# memory tokens (like [cls]) from Memory Transformers paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mnum_memory_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_memory_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cao Hoc/Research Methodology/Im2Tex/running/thanh_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context, mask, context_mask, mems, return_hiddens)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: context must be passed in if cross_attend is set to True"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(decoder,               # model being run\n",
    "                  # model input (or a tuple for multiple inputs)context=encoder(im)\n",
    "                  (start_tokens, max_seq_len, eos_token, temperature, context),\n",
    "                  # where to save the model (can be a file or file-like object)\n",
    "                  \"latex_decode.onnx\",\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names=['input'],   # the model's input names\n",
    "                  output_names=['output'],  # the model's output names\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},    # variable length axes\n",
    "                                'output': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6667, 0.9323, 0.3032, 0.6710],\n",
       "         [0.5033, 0.8415, 0.3883, 0.9128],\n",
       "         [0.5966, 0.7419, 0.4599, 0.8341]], grad_fn=<TanhBackward0>),\n",
       " tensor([[0.6667, 0.9323, 0.3032, 0.6710],\n",
       "         [0.5033, 0.8415, 0.3883, 0.9128],\n",
       "         [0.5966, 0.7419, 0.4599, 0.8341]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "context= torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, context))\n",
    "print(traced_cell)\n",
    "traced_cell(x, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0364, -0.0342, -0.0122,  ..., -0.0185, -0.0135,  0.0023],\n",
       "         [ 0.1347,  0.0388,  0.2878,  ..., -0.0078, -0.0010,  0.0016],\n",
       "         [-0.2407,  0.0618,  0.1151,  ..., -0.1899, -0.0018,  0.0011],\n",
       "         ...,\n",
       "         [ 0.0145,  0.0773, -0.1272,  ..., -0.1472,  0.0039,  0.0012],\n",
       "         [ 0.2215,  0.1679, -0.2498,  ..., -0.0849,  0.0022,  0.0022],\n",
       "         [ 0.2342,  0.1548, -0.2931,  ...,  0.0595,  0.0006,  0.0014]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_cell = torch.jit.trace(encoder_model, im.to(device))\n",
    "print(traced_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of (tensor([[1]]), 512, 2, 0.2, <function top_k at 0x7f967ad11a70>, 0.9, tensor([[[-0.0364, -0.0342, -0.0122,  ..., -0.0185, -0.0135,  0.0023],\n         [ 0.1347,  0.0388,  0.2878,  ..., -0.0078, -0.0010,  0.0016],\n         [-0.2407,  0.0618,  0.1151,  ..., -0.1899, -0.0018,  0.0011],\n         ...,\n         [ 0.0145,  0.0773, -0.1272,  ..., -0.1472,  0.0039,  0.0012],\n         [ 0.2215,  0.1679, -0.2498,  ..., -0.0849,  0.0022,  0.0022],\n         [ 0.2342,  0.1548, -0.2931,  ...,  0.0595,  0.0006,  0.0014]]]))\n:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153433/1639414394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m traced_cell = torch.jit.trace(decoder_model,\n\u001b[0;32m----> 2\u001b[0;31m                               (start_tokens, seq_len, eos_token, temperature, filter_logits_fn, filter_thres, context))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         )\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    963\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 \u001b[0margument_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             )\n\u001b[1;32m    967\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of (tensor([[1]]), 512, 2, 0.2, <function top_k at 0x7f967ad11a70>, 0.9, tensor([[[-0.0364, -0.0342, -0.0122,  ..., -0.0185, -0.0135,  0.0023],\n         [ 0.1347,  0.0388,  0.2878,  ..., -0.0078, -0.0010,  0.0016],\n         [-0.2407,  0.0618,  0.1151,  ..., -0.1899, -0.0018,  0.0011],\n         ...,\n         [ 0.0145,  0.0773, -0.1272,  ..., -0.1472,  0.0039,  0.0012],\n         [ 0.2215,  0.1679, -0.2498,  ..., -0.0849,  0.0022,  0.0022],\n         [ 0.2342,  0.1548, -0.2931,  ...,  0.0595,  0.0006,  0.0014]]]))\n:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type function."
     ]
    }
   ],
   "source": [
    "traced_cell = torch.jit.trace(decoder_model,\n",
    "                              (start_tokens, seq_len, eos_token, temperature, filter_logits_fn, filter_thres, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
